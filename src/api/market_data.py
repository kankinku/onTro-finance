import os
import json
import logging
from typing import List, Dict, Any, Optional
from datetime import datetime, timedelta
import yfinance as yf
from fredapi import Fred
import numpy as np
from config.settings import settings
from src.core.logger import logger

# ìºì‹œ íŒŒì¼ ê²½ë¡œëŠ” settingsì—ì„œ ê´€ë¦¬
CACHE_DIR = settings.CACHE_DIR
CACHE_FILE = str(settings.market_cache_path)
CUSTOM_MAPPING_FILE = str(CACHE_DIR / "custom_market_mapping.json")


class MarketDataProvider:
    """
    [Data Layer]
    Fetches market data from YFinance and FRED.
    Implements local JSON caching and Dynamic Source Discovery.
    """
    
    def __init__(self):
        self.fred = None
        if settings.FRED_API_KEY:
            try:
                self.fred = Fred(api_key=settings.FRED_API_KEY)
            except Exception as e:
                logger.error(f"Failed to initialize FRED API: {e}")
        
        # 1. Base Mapping (Hardcoded)
        self.base_mapping = {
            "TERM_BASE_RATE": {"source": "fred", "ticker": "DFF", "name": "Effective Federal Funds Rate"},
            "TERM_INFLATION": {"source": "fred", "ticker": "T10YIE", "name": "10-Year Breakeven Inflation Rate"},
            "TERM_EXCHANGE_RATE": {"source": "yf", "ticker": "KRW=X", "name": "USD/KRW Exchange Rate"},
            "TERM_TREASURY_YIELD": {"source": "yf", "ticker": "^TNX", "name": "Treasury Yield 10 Years"},
            "TERM_UST": {"source": "yf", "ticker": "^TNX", "name": "Treasury Yield 10 Years"}, 
            "TERM_LIQUIDITY": {"source": "fred", "ticker": "RRPONTSYD", "name": "Overnight Reverse Repurchase Agreements"},
            "TERM_ASSET_PRICE": {"source": "yf", "ticker": "^GSPC", "name": "S&P 500"},
            "TERM_CORP_BOND": {"source": "yf", "ticker": "LQD", "name": "iShares iBoxx $ Inv Grade Corporate Bond ETF"},
            "TERM_CREDIT_SPREAD": {"source": "fred", "ticker": "BAMLC0A0CM", "name": "ICE BofA US Corp Master Option-Adjusted Spread"},
            "TERM_TREASURY_DEMAND": {"source": "yf", "ticker": "^TNX", "name": "Treasury Yield (Inverse Proxy)"},
        }

        # 2. Load Custom Mapping (Dynamic)
        self.custom_mapping = self._load_custom_mapping()
        
        # 3. Merge Mappings (Priority: Custom > Base)
        self.mapping = {**self.base_mapping, **self.custom_mapping}

        # Dashboard targets config
        self.dashboard_targets = [
            {"id": "TGA", "source": "fred", "ticker": "WTREGEN", "title": "ìž¬ë¬´ë¶€ ì¼ë°˜ê³„ì • (TGA)", "desc": "ë¯¸êµ­ ì •ë¶€ì˜ ë¹„ìƒê¸ˆ í†µìž¥ ìž”ê³ ìž…ë‹ˆë‹¤.", "interpret_up": "ì‹œì¤‘ ìœ ë™ì„± í¡ìˆ˜ (ë¶€ì •ì )", "interpret_down": "ì‹œì¤‘ ìœ ë™ì„± ë°©ì¶œ (ê¸ì •ì )"},
            {"id": "RESERVES", "source": "fred", "ticker": "TOTRESNS", "title": "ì§€ê¸‰ì¤€ë¹„ê¸ˆ (Reserves)", "desc": "ì‹œì¤‘ ì€í–‰ë“¤ì´ ì—°ì¤€ì— ì˜ˆì¹˜í•´ë‘” í˜„ê¸ˆ ì´ì•¡ìž…ë‹ˆë‹¤.", "interpret_up": "ì€í–‰ ëŒ€ì¶œì—¬ë ¥ ì¦ê°€ (ê¸ì •ì )", "interpret_down": "ì€í–‰ ëŒ€ì¶œì—¬ë ¥ ê°ì†Œ (ë¶€ì •ì )"},
            {"id": "RRP", "source": "fred", "ticker": "RRPONTSYD", "title": "ì—­ë ˆí¬ (ON RRP)", "desc": "ë‹¨ê¸° ìžê¸ˆì´ ë¨¸ë¬´ëŠ” íŒŒí‚¹ í†µìž¥ ìž”ê³ ìž…ë‹ˆë‹¤.","interpret_up": "ì‹œìž¥ ìžê¸ˆ ê²½ìƒ‰ ê°€ëŠ¥ì„±", "interpret_down": "ì‹œìž¥ìœ¼ë¡œ ìžê¸ˆ ì´ë™ (ê¸ì •ì )"},
            {"id": "YIELD_10Y", "source": "yf", "ticker": "^TNX", "title": "ë¯¸êµ­ì±„ 10ë…„ë¬¼ ê¸ˆë¦¬", "desc": "ì „ ì„¸ê³„ ìžì‚° ê°€ê²©ì˜ ê¸°ì¤€ì´ ë˜ëŠ” ê¸ˆë¦¬ìž…ë‹ˆë‹¤.", "interpret_up": "ìžì‚° ê°€ì¹˜ í•˜ë½ ì••ë ¥", "interpret_down": "ìžì‚° ê°€ì¹˜ ìƒìŠ¹ ìš”ì¸"},
            {"id": "DFF", "source": "fred", "ticker": "DFF", "title": "ì—°ì¤€ ê¸°ì¤€ê¸ˆë¦¬ (FFR)", "desc": "ë¯¸êµ­ ì¤‘ì•™ì€í–‰ì˜ ì •ì±… ê¸ˆë¦¬ìž…ë‹ˆë‹¤.", "interpret_up": "ê¸´ì¶• ì •ì±… (ìœ ë™ì„± ì¶•ì†Œ)", "interpret_down": "ì™„í™” ì •ì±… (ìœ ë™ì„± ê³µê¸‰)"},
        ]
        
        # Load Cache
        self.cache = self._load_cache()

    def _load_cache(self) -> Dict[str, Any]:
        if os.path.exists(CACHE_FILE):
            try:
                with open(CACHE_FILE, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except Exception as e:
                logger.error(f"Failed to load cache: {e}")
        return {}

    def _save_cache(self):
        try:
            with open(CACHE_FILE, 'w', encoding='utf-8') as f:
                json.dump(self.cache, f, ensure_ascii=False, indent=2)
        except Exception as e:
            logger.error(f"Failed to save cache: {e}")

    def _load_custom_mapping(self) -> Dict[str, Any]:
        if os.path.exists(CUSTOM_MAPPING_FILE):
            try:
                with open(CUSTOM_MAPPING_FILE, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except Exception as e:
                logger.error(f"Failed to load custom mapping: {e}")
        return {}

    def _save_custom_mapping(self):
        try:
            with open(CUSTOM_MAPPING_FILE, 'w', encoding='utf-8') as f:
                json.dump(self.custom_mapping, f, ensure_ascii=False, indent=2)
        except Exception as e:
            logger.error(f"Failed to save custom mapping: {e}")

    def search_and_register_ticker(self, keyword: str) -> Optional[str]:
        """
        Searches API (FRED priority) for the keyword.
        If found, registers it to custom_mapping and fetches data.
        Returns the registered Term ID or None.
        """
        if not self.fred:
            logger.warning("FRED API not available for discovery.")
            return None
        
        logger.info(f"ðŸ”Ž Auto-Discovering Data for keyword: '{keyword}'...")
        
        try:
            # 1. Search FRED
            search_results = self.fred.search(keyword, limit=5, order_by='popularity', sort_order='desc')
            if search_results is None or search_results.empty:
                logger.info(f"No results found in FRED for {keyword}")
                return None
            
            # Pick the top result
            top_result = search_results.iloc[0]
            series_id = top_result.name # FRED returns Series ID as index (usually) or 'id' column
            title = top_result['title']
            
            # Generate a Term ID
            clean_key = keyword.upper().replace(" ", "_")
            term_id = f"TERM_{clean_key}"
            
            # Register
            new_entry = {
                "source": "fred",
                "ticker": series_id,
                "name": title
            }
            
            self.custom_mapping[term_id] = new_entry
            self.mapping[term_id] = new_entry # Update current runtime mapping
            
            self._save_custom_mapping()
            logger.info(f"âœ… Registered New Source: {term_id} -> {title} ({series_id})")
            
            # Trigger Fetch immediately for this one
            self.initialize_data(specific_ticker=series_id) 
            
            return term_id
            
        except Exception as e:
            logger.error(f"Discovery Failed: {e}")
            return None

    def initialize_data(self, specific_ticker=None):
        """
        [Sync Process]
        Checks local cache for missing data up to today.
        Fetches only missing periods from APIs.
        Finally, runs LLM analysis on the fresh data (unless specific_ticker is set).
        """
        logger.info(f"Initializing market data... (Target: {specific_ticker if specific_ticker else 'ALL'})")
        
        # Check all tickers used in mapping and dashboard
        all_tickers = []
        
        # Add Dashboard Tickers
        for t in self.dashboard_targets:
            all_tickers.append(t)
            
        # Add Method 2 tickers (Mapping)
        for key, val in self.mapping.items():
            # Check if already added
            found = False
            for existing in all_tickers:
                if existing['ticker'] == val['ticker']:
                    found = True
                    break
            if not found:
                all_tickers.append({"id": key, "source": val["source"], "ticker": val["ticker"]})

        # Filter if specific ticker requested
        if specific_ticker:
            all_tickers = [t for t in all_tickers if t['ticker'] == specific_ticker]
            if not all_tickers:
                logger.warning(f"Ticker {specific_ticker} not found in configuration.")
                return

        today = datetime.now().date()
        
        for item in all_tickers:
            ticker = item['ticker']
            source = item['source']
            
            # Cache Key
            cache_key = f"{source}:{ticker}"
            
            # Get last update date from cache
            if cache_key not in self.cache:
                self.cache[cache_key] = {"last_updated": "2023-01-01", "history": {}}
            
            last_dt_str = self.cache[cache_key]["last_updated"]
            last_dt = datetime.strptime(last_dt_str, "%Y-%m-%d").date()
            
            # Calculate days gap
            # Don't fetch if updated today
            if last_dt >= today:
                continue
                
            start_date = last_dt + timedelta(days=1)
            
            # Fetch
            logger.info(f"Fetching {ticker} from {start_date} to {today}...")
            try:
                df = None
                if source == "yf":
                    # Yfinance
                    df = yf.Ticker(ticker).history(start=start_date.strftime("%Y-%m-%d"), end=today.strftime("%Y-%m-%d") )
                    if df.empty:
                        # Fallback for generic 'recent' fetch if specific date fails
                         df = yf.Ticker(ticker).history(period="5d")
                    else:
                         df = df[['Close']]

                elif source == "fred":
                    if self.fred:
                        s = self.fred.get_series(ticker, observation_start=start_date.strftime("%Y-%m-%d"))
                        if not s.empty:
                            df = s.to_frame(name='Close')

                if df is not None and not df.empty:
                    # Merge into cache
                    history = self.cache[cache_key]["history"]
                    for idx, row in df.iterrows():
                        # idx is Timestamp
                        date_str = idx.strftime("%Y-%m-%d")
                        val = float(row['Close'])
                        if not np.isnan(val):
                            history[date_str] = val
                    
                    # Update metadata
                    sorted_dates = sorted(history.keys())
                    if sorted_dates:
                        self.cache[cache_key]["last_updated"] = sorted_dates[-1]
                        
            except Exception as e:
                logger.error(f"Error updating {ticker}: {e}")
        
        self._save_cache()
        logger.info("Market data synchronization complete.")
        
        # Trigger LLM Analysis ONLY when doing full init
        if not specific_ticker:
            self.analyze_market_with_llm()

    def analyze_market_with_llm(self):
        """
        Summarize market conditions using Ollama (local LLM).
        Follows a strict 'Cynical & Structural' analysis persona.
        """
        logger.info("Running LLM Market Analysis...")
        import requests

        # 1. Gather Summary Data
        data_snapshot = []
        for t in self.dashboard_targets:
            res = self._get_cached_metric(t["source"], t["ticker"], t["title"])
            if res:
                data_snapshot.append(f"- {t['title']}: {res['value']:.2f} (Weekly Change: {res['change_1w']:+.2f}%)")
        
        data_text = "\n".join(data_snapshot)
        
        prompt = f"""
        ë‹¹ì‹ ì€ ëƒ‰ì² í•œ ê±°ì‹œê²½ì œ ë¶„ì„ê°€ìž…ë‹ˆë‹¤. ì•„ëž˜ ì‹œìž¥ ë°ì´í„°ë¥¼ ë³´ê³  ìœ ë™ì„± ê´€ì ì—ì„œ í•œêµ­ì–´ë¡œ 3ì¤„ ìš”ì•½ì„ ìž‘ì„±í•˜ì‹­ì‹œì˜¤.
        
        [ì œì•½ ì‚¬í•­]
        1. ì–¸ì–´: í•œêµ­ì–´ë§Œ ì‚¬ìš©í•  ê²ƒ (í•„ìš” ì‹œ ì˜ì–´ ê¸ˆìœµ ìš©ì–´ ë³‘ê¸° ê°€ëŠ¥). ê·¸ ì™¸ ì–¸ì–´ ì ˆëŒ€ ê¸ˆì§€.
        2. í˜•ì‹: ë§ˆí¬ë‹¤ìš´(Markdown), ë³¼ë“œì²´(**), ì´ëª¨ì§€ ì‚¬ìš© ê¸ˆì§€. ì˜¤ë¡œì§€ í‰ë¬¸ í…ìŠ¤íŠ¸ë¡œë§Œ ìž‘ì„±í•  ê²ƒ.
        3. ì–´ì¡°: ê°ì •ì  ë¯¸ì‚¬ì—¬êµ¬ ë°°ì œ. ëƒ‰ì •í•˜ê³  ë‹¨í˜¸í•˜ê²Œ (~í•¨. ~ìž„. ì²´ë¡œ ì¢…ê²°).

        [í˜„ìž¬ ì‹œìž¥ ë°ì´í„°]
        {data_text}
        
        [ìž‘ì„± ê°€ì´ë“œ]
        - ë‚´ìš©: TGA, ì§€ì¤€ê¸ˆ(Reservs), ê¸ˆë¦¬ë¥¼ ì¢…í•©í•˜ì—¬ ì‹¤ì œ ì‹œìž¥ ìœ ë™ì„±ì´ ëŠ˜ì—ˆëŠ”ì§€ ì¤„ì—ˆëŠ”ì§€ íŒë‹¨í•  ê²ƒ.
        - í•µì‹¬: ë‹¨ìˆœ ìˆ˜ì¹˜ ë‚˜ì—´ì´ ì•„ë‹ˆë¼ 'ì˜ë¯¸'ë¥¼ í•´ì„í•  ê²ƒ.
        
        [ì¶œë ¥ ì˜ˆì‹œ]
        1. TGA ì¦ê°€ì™€ QT ì§€ì†ìœ¼ë¡œ ì‹¤ì§ˆ ìœ ë™ì„±ì€ ê°ì†Œí•¨.
        2. êµ­ì±„ ê¸ˆë¦¬ ìƒìŠ¹ì€ ì•ˆì „ ìžì‚° ì„ í˜¸ ì‹¬ë¦¬ê°€ ì•½í™”ë˜ì—ˆìŒì„ ì‹œì‚¬.
        3. ë‹¨ê¸°ì  ë°˜ë“±ì´ ìžˆë”ë¼ë„ êµ¬ì¡°ì  ìœ ë™ì„± í™˜ê²½ì€ ì—¬ì „ížˆ ê¸´ì¶•ì ìž„.
        """

        try:
            resp = requests.post(
                "http://localhost:11434/api/generate",
                json={
                    "model": settings.OLLAMA_MODEL,
                    "prompt": prompt,
                    "stream": False,
                    "options": {"temperature": 0.2}
                },
                timeout=20
            )
            if resp.status_code == 200:
                result = resp.json().get("response", "").strip()
                self.cache["market_insight"] = result
                self._save_cache()
                logger.info("LLM Analysis Complete & Cached.")
            else:
                logger.error(f"Ollama Error: {resp.text}")
        except Exception as e:
            logger.error(f"Failed to run LLM analysis: {e}")

    def get_market_indicator(self, term_id: str) -> Optional[Dict[str, Any]]:
        if term_id not in self.mapping:
            return None
        config = self.mapping[term_id]
        return self._get_cached_metric(config["source"], config["ticker"], config["name"])

    def get_metric_history(self, source: str, ticker: str) -> List[Dict[str, Any]]:
        """
        Returns historical data for charting.
        Sorted by date ascending.
        """
        cache_key = f"{source}:{ticker}"
        if cache_key not in self.cache:
            return []
        
        history = self.cache[cache_key].get("history", {})
        # Sort by date
        sorted_items = sorted(history.items())
        
        # Convert to list of dicts
        return [{"date": k, "value": v} for k, v in sorted_items]

    def analyze_metric_detail(self, source: str, ticker: str, title: str) -> str:
        """
        Generates a deep-dive report for a specific metric using LLM.
        """
        import requests
        
        # Get recent history (last 30 days) to show trend context
        history = self.get_metric_history(source, ticker)
        recent_data = history[-30:] if history else []
        
        if not recent_data:
            return "ë°ì´í„°ê°€ ë¶€ì¡±í•˜ì—¬ ë¶„ì„í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

        start_val = recent_data[0]['value']
        end_val = recent_data[-1]['value']
        change_pct = ((end_val - start_val) / start_val) * 100 if start_val != 0 else 0
        
        prompt = f"""
        ê¸ˆìœµ ì „ë¬¸ê°€ë¡œì„œ ë‹¤ìŒ ì§€í‘œ({title})ë¥¼ ë¶„ì„í•˜ì—¬ ë³´ê³ í•´.

        [ê°•ë ¥í•œ ì œì•½ ì‚¬í•­]
        1. ì œëª©/ì†Œì œëª©(ì œ1ìž¥, AI ë¦¬í¬íŠ¸ ë“±) ì ˆëŒ€ ê¸ˆì§€. ë°”ë¡œ ë³¸ë¬¸ ì‹œìž‘í•  ê²ƒ.
        2. ë³¼ë“œì²´(**), ì´ëª¨ì§€, ë§ˆí¬ë‹¤ìš´ í—¤ë”(#) ì ˆëŒ€ ì‚¬ìš© ê¸ˆì§€.
        3. ì˜¤ë¡œì§€ í‰ë¬¸ í…ìŠ¤íŠ¸ì™€ í•˜ë‚˜ì˜ 'ë§ˆí¬ë‹¤ìš´ í‘œ'ë¡œë§Œ êµ¬ì„±í•  ê²ƒ.
        4. ì–¸ì–´: í•œêµ­ì–´ë§Œ ì‚¬ìš© (ì˜ì–´ ë‹¨ì–´ ìµœì†Œí™”).

        [ë¶„ì„ ëŒ€ìƒ]
        - ì§€í‘œ: {title}
        - í˜„ìž¬ê°’: {end_val:,.2f}
        - ë³€ë™: {change_pct:+.2f}%
        - ì¶”ì´: {recent_data[-5:]}

        [ì¶œë ¥ ìˆœì„œ ë° ê°€ì´ë“œ]
        1. ìµœê·¼ ë°ì´í„° 5ê°œë¥¼ 'ë‚ ì§œ | ê°’' í˜•íƒœì˜ ë§ˆí¬ë‹¤ìš´ í‘œë¡œ ìž‘ì„±.
        2. ì´ì–´ì„œ ë°”ë¡œ ë¶„ì„ ë‚´ìš© ìž‘ì„± (3ì¤„ ë‚´ì™¸).
        3. ì—°ê´€ ìžì‚° íŒŒê¸‰ íš¨ê³¼ë¥¼ 2ì¤„ë¡œ ìš”ì•½.
        """
        
        try:
            resp = requests.post(
                "http://localhost:11434/api/generate",
                json={
                    "model": settings.OLLAMA_MODEL,
                    "prompt": prompt,
                    "stream": False,
                    "options": {"temperature": 0.3}
                },
                timeout=25
            )
            if resp.status_code == 200:
                return resp.json().get("response", "ë¶„ì„ ì‹¤íŒ¨").strip()
            return f"LLM ì˜¤ë¥˜: {resp.status_code}"
        except Exception as e:
            return f"ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}"

    def get_dashboard_summary(self) -> Dict[str, Any]:
        """
        Returns structured dashboard data:
        {
            "insight": "LLM Analysis Text...",
            "cards": [ ... ]
        }
        """
        cards = []
        for t in self.dashboard_targets:
            data = self._get_cached_metric(t["source"], t["ticker"], t["title"])
            if not data:
                continue
            
            # Calculate Interpretation
            change_val = data['change_1w'] # Use 1W for robust trend
            
            interpretation = t["interpret_up"] if change_val >= 0 else t["interpret_down"]
            is_good = (t["id"] in ["RESERVES", "DFF"] and change_val > 0) or (t["id"] not in ["RESERVES", "DFF"] and change_val < 0)

            cards.append({
                "id": t["id"], # Add ID for frontend mapping
                "title": t["title"],
                "value": f"{data['value']:,.2f}",
                "change": f"{data['change_1w']:+.2f}%", 
                "desc": t["desc"],
                "interpretation": f"ë³€ë™ ì˜ë¯¸: {interpretation}",
                "is_positive": bool(is_good),
                "source": t["source"],
                "ticker": t["ticker"]
            })
            
        insight = self.cache.get("market_insight", "ì‹œìž¥ ë¶„ì„ ë°ì´í„°ê°€ ì¶©ë¶„í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        
        return {
            "insight": insight,
            "cards": cards
        }

    def _get_cached_metric(self, source, ticker, name) -> Optional[Dict[str, Any]]:
        cache_key = f"{source}:{ticker}"
        if cache_key not in self.cache:
            return None
        
        hist = self.cache[cache_key].get("history", {})
        if not hist:
            return None
            
        # Sort dates
        dates = sorted(hist.keys())
        latest_date = dates[-1]
        latest_val = hist[latest_date]
        
        # Helper for % change
        def get_pct_change(days_ago):
            target_dt = datetime.strptime(latest_date, "%Y-%m-%d") - timedelta(days=days_ago)
            # Find closest date <= target_dt
            target_str = target_dt.strftime("%Y-%m-%d")
            
            found_val = None
            # Iterate backwards from latest
            for d in reversed(dates):
                if d <= target_str:
                    found_val = hist[d]
                    break
            
            if found_val is None:
                found_val = hist[dates[0]] # Oldest available
            
            if found_val == 0: return 0.0
            return ((latest_val - found_val) / found_val) * 100.0

        change_1d = get_pct_change(1)
        change_1w = get_pct_change(7)
        change_1m = get_pct_change(30)
        
        trend = "STABLE"
        if change_1w > 0.5: trend = "UP"
        elif change_1w < -0.5: trend = "DOWN"

        return {
            "indicator": name,
            "value": float(latest_val),
            "unit": "",
            "change_1d": float(change_1d),
            "change_1w": float(change_1w),
            "change_1m": float(change_1m),
            "trend": trend,
            "data_source": f"{source.upper()}:{ticker}",
            "timestamp": latest_date
        }

    def check_trend_alignment(self, term_id: str, expected_direction: str) -> float:
        data = self.get_market_indicator(term_id)
        if not data:
            return 0.5

        trend = data['trend']
        
        # Inversion logic
        if term_id == "TERM_TREASURY_DEMAND" and "TNX" in data['data_source']:
             if trend == "UP": trend = "DOWN"
             elif trend == "DOWN": trend = "UP"

        if expected_direction == "INCREASE":
            return 1.0 if trend == "UP" else (0.0 if trend == "DOWN" else 0.5)
        elif expected_direction == "DECREASE":
            return 1.0 if trend == "DOWN" else (0.0 if trend == "UP" else 0.5)
            
        return 0.5
